package jvn

import (
	"bytes"
	"encoding/xml"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/cheggaaa/pb"
	c "github.com/kotakanbe/go-cve-dictionary/config"
	log "github.com/kotakanbe/go-cve-dictionary/log"
	"github.com/kotakanbe/go-cve-dictionary/util"
)

type rdf struct {
	Items []Item `xml:"item"`
}

// Item ... http://jvndb.jvn.jp/apis/getVulnOverviewList_api.html
type Item struct {
	About       string       `xml:"about,attr"`
	Title       string       `xml:"title"`
	Link        string       `xml:"link"`
	Description string       `xml:"description"`
	Publisher   string       `xml:"publisher"`
	Identifier  string       `xml:"identifier"`
	References  []references `xml:"references"`
	CpeItem     []cpeItem    `xml:"cpe-item"`
	Cvss        Cvss         `xml:"cvss"`
	Date        string       `xml:"date"`
	Issued      string       `xml:"issued"`
	Modified    string       `xml:"modified"`
}

type cpeItem struct {
	// cpe:/a:mysql:mysql
	Name string `xml:"name,attr"`
	// MySQL
	Title string `xml:"title"`
	// MySQL AB
	Vname string `xml:"vname"`
}

type references struct {
	ID     string `xml:"id,attr"`
	Source string `xml:"source,attr"`
	URL    string `xml:",chardata"`
}

// Cvss ... CVSS
type Cvss struct {
	Score    string `xml:"score,attr"`
	Severity string `xml:"severity,attr"`
	Vector   string `xml:"vector,attr"`
	Version  string `xml:"version,attr"`
}

// FetchJvn fetches vulnerability information from JVN
func FetchJvn(years []int) (items []Item, err error) {
	urls := makeJvnURLs(years)
	items, err = fetchJvnConcurrently(urls)
	if err != nil {
		return items, fmt.Errorf(
			"Failed to fetch cve data from JVN. err: %s", err)
	}
	return
}

func makeJvnURLs(years []int) (urls []string) {
	latestFeeds := []string{
		"http://jvndb.jvn.jp/ja/rss/jvndb_new.rdf",
		"http://jvndb.jvn.jp/ja/rss/jvndb.rdf",
	}

	if len(years) == 0 {
		return latestFeeds
	}

	urlFormat := "http://jvndb.jvn.jp/ja/rss/years/jvndb_%d.rdf"
	for _, year := range years {
		urls = append(urls, fmt.Sprintf(urlFormat, year))

		thisYear := time.Now().Year()
		if year == thisYear {
			urls = append(urls, latestFeeds...)
		}
	}
	return
}

func fetchJvnConcurrently(urls []string) (allItems []Item, err error) {
	reqChan := make(chan string, len(urls))
	resChan := make(chan []Item, len(urls))
	errChan := make(chan error, len(urls))
	defer close(reqChan)
	defer close(resChan)
	defer close(errChan)

	for _, url := range urls {
		log.Infof("Fetching... %s", url)
	}

	var pool *pb.Pool
	bars := map[string]*pb.ProgressBar{}
	go func() {
		for _, url := range urls {
			prefix := filepath.Base(url) + ":"
			bars[url] = pb.New(util.GetFileSize(url)).SetUnits(pb.U_BYTES).Prefix(prefix)

			if pool == nil {
				if pool, err = pb.StartPool(bars[url]); err != nil {
					log.Warn(err)
				}
			} else {
				pool.Add(bars[url])
			}
			reqChan <- url
		}
	}()

	concurrency := len(urls)
	tasks := util.GenWorkers(concurrency)
	wg := new(sync.WaitGroup)
	for range urls {
		wg.Add(1)
		tasks <- func() {
			select {
			case url := <-reqChan:
				items, err := fetchJvn(url, bars[url])
				wg.Done()
				if err != nil {
					errChan <- err
					return
				}
				resChan <- items
			}
			return
		}
	}
	wg.Wait()
	pool.Stop()

	errs := []error{}
	timeout := time.After(10 * 60 * time.Second)
	for range urls {
		select {
		case items := <-resChan:
			allItems = append(allItems, items...)
		case err := <-errChan:
			errs = append(errs, err)
		case <-timeout:
			return allItems, fmt.Errorf("Timeout Fetching Jvn")
		}
	}
	if 0 < len(errs) {
		return allItems, fmt.Errorf("%s", errs)
	}
	return allItems, nil
}

func fetchJvn(targetURL string, bar *pb.ProgressBar) (items []Item, err error) {
	var errs []error
	var proxyURL *url.URL
	var resp *http.Response

	httpCilent := &http.Client{}
	if c.Conf.HTTPProxy != "" {
		if proxyURL, err = url.Parse(c.Conf.HTTPProxy); err != nil {
			return nil, fmt.Errorf("Failed to parse proxy url: %s", err)
		}
		httpCilent = &http.Client{Transport: &http.Transport{Proxy: http.ProxyURL(proxyURL)}}
	}

	if resp, err = httpCilent.Get(targetURL); err != nil {
		fmt.Fprintf(os.Stderr, "Download failed: %v\n", err)
		os.Exit(1)
	}
	defer resp.Body.Close()

	buf := bytes.NewBuffer(nil)
	bar.Start()
	rd := bar.NewProxyReader(resp.Body)
	io.Copy(buf, rd)

	if len(errs) > 0 || resp == nil || resp.StatusCode != 200 {
		return items, fmt.Errorf(
			"HTTP error. errs: %v, url: %s", errs, targetURL)
	}

	var rdf rdf
	if err = xml.Unmarshal(buf.Bytes(), &rdf); err != nil {
		return items, fmt.Errorf(
			"Failed to unmarshal. url: %s, err: %s", targetURL, err)
	}
	items = append(items, rdf.Items...)
	bar.Finish()
	return
}
