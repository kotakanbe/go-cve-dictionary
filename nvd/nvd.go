package nvd

import (
	"bytes"
	"compress/gzip"
	"encoding/xml"
	"fmt"
	"io"
	"io/ioutil"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/cheggaaa/pb"
	c "github.com/kotakanbe/go-cve-dictionary/config"
	log "github.com/kotakanbe/go-cve-dictionary/log"
	"github.com/kotakanbe/go-cve-dictionary/util"
)

// Nvd is array of Entry
type Nvd struct {
	Entries []Entry `xml:"entry"`
}

// Entry is Root Element
type Entry struct {
	CveID            string      `xml:"id,attr" json:"id"`
	PublishedDate    time.Time   `xml:"published-datetime"`
	LastModifiedDate time.Time   `xml:"last-modified-datetime"`
	Cvss             Cvss        `xml:"cvss>base_metrics" json:"cvss"`
	Products         []string    `xml:"vulnerable-software-list>product"` //CPE
	Summary          string      `xml:"summary"`
	References       []Reference `xml:"references"`
	Cwe              Cwe         `xml:"cwe"`
}

// Cvss is Cvss Score
type Cvss struct {
	Score                 string    `xml:"score"`
	AccessVector          string    `xml:"access-vector"`
	AccessComplexity      string    `xml:"access-complexity"`
	Authentication        string    `xml:"authentication"`
	ConfidentialityImpact string    `xml:"confidentiality-impact"`
	IntegrityImpact       string    `xml:"integrity-impact"`
	AvailabilityImpact    string    `xml:"availability-impact"`
	Source                string    `xml:"source"`
	GeneratedOnDate       time.Time `xml:"generated-on-datetime"`
}

// Cwe has Cwe ID
type Cwe struct {
	ID string `xml:"id,attr"`
}

// Reference is additional information about the CVE
type Reference struct {
	Type   string `xml:"reference_type,attr"`
	Source string `xml:"source"`
	Link   Link   `xml:"reference"`
}

// Link is additional information about the CVE
type Link struct {
	Value string `xml:",chardata" json:"value"`
	Href  string `xml:"href,attr" json:"href"`
}

// FetchFiles Fetch CVE vulnerability informatino from JVN
func FetchFiles(years []int) (entries []Entry, err error) {
	urls := makeFeedURLs(years)
	nvds, err := fetchFeedFileConcurrently(urls)
	if err != nil {
		return entries,
			fmt.Errorf("Failed to fetch cve data from NVD. err: %s", err)
	}
	for _, nvd := range nvds {
		entries = append(entries, nvd.Entries...)
	}
	return entries, nil
}

func makeFeedURLs(years []int) (urls []string) {
	//  http://static.nvd.nist.gov/feeds/xml/cve/nvdcve-2.0-2016.xml.gz
	formatTemplate := "https://static.nvd.nist.gov/feeds/xml/cve/nvdcve-2.0-%d.xml.gz"
	for _, year := range years {
		urls = append(urls, fmt.Sprintf(formatTemplate, year))
	}
	return
}

func fetchFeedFileConcurrently(urls []string) (nvds []Nvd, err error) {
	reqChan := make(chan string, len(urls))
	resChan := make(chan Nvd, len(urls))
	errChan := make(chan error, len(urls))
	defer close(reqChan)
	defer close(resChan)
	defer close(errChan)

	for _, url := range urls {
		log.Infof("Fetching... %s", url)
	}

	var pool *pb.Pool
	bars := map[string]*pb.ProgressBar{}
	go func() {
		for _, url := range urls {
			prefix := filepath.Base(url) + ":"
			bars[url] = pb.New(util.GetFileSize(url)).SetUnits(pb.U_BYTES).Prefix(prefix)

			if pool == nil {
				if pool, err = pb.StartPool(bars[url]); err != nil {
					log.Warn(err)
				}
			} else {
				pool.Add(bars[url])
			}
			reqChan <- url
		}
	}()

	concurrency := len(urls)
	tasks := util.GenWorkers(concurrency)
	wg := new(sync.WaitGroup)
	for range urls {
		wg.Add(1)
		tasks <- func() {
			select {
			case url := <-reqChan:
				nvd, err := fetchFeedFile(url, bars[url])
				wg.Done()
				if err != nil {
					errChan <- err
					return
				}
				resChan <- nvd
			}
			return
		}
	}
	wg.Wait()
	pool.Stop()

	errs := []error{}
	timeout := time.After(10 * 60 * time.Second)
	for range urls {
		select {
		case nvd := <-resChan:
			nvds = append(nvds, nvd)
		case err := <-errChan:
			errs = append(errs, err)
		case <-timeout:
			return nvds, fmt.Errorf("Timeout Fetching Nvd")
		}
	}
	if 0 < len(errs) {
		return nvds, fmt.Errorf("%s", errs)
	}
	return nvds, nil
}

func fetchFeedFile(targetURL string, bar *pb.ProgressBar) (nvd Nvd, err error) {
	var errs []error
	var proxyURL *url.URL
	var resp *http.Response

	httpCilent := &http.Client{}
	if c.Conf.HTTPProxy != "" {
		if proxyURL, err = url.Parse(c.Conf.HTTPProxy); err != nil {
			return nvd, fmt.Errorf("Failed to parse proxy url: %s", err)
		}
		httpCilent = &http.Client{Transport: &http.Transport{Proxy: http.ProxyURL(proxyURL)}}
	}

	if resp, err = httpCilent.Get(targetURL); err != nil {
		fmt.Fprintf(os.Stderr, "Download failed: %v\n", err)
		os.Exit(1)
	}
	defer resp.Body.Close()

	buf := bytes.NewBuffer(nil)
	bar.Start()
	rd := bar.NewProxyReader(resp.Body)
	io.Copy(buf, rd)

	if len(errs) > 0 || resp == nil || resp.StatusCode != 200 {
		return nvd, fmt.Errorf(
			"HTTP error. errs: %v, url: %s", errs, targetURL)
	}

	reader, err := gzip.NewReader(buf)
	defer reader.Close()
	if err != nil {
		return nvd, fmt.Errorf(
			"Failed to decompress NVD feedfile. url: %s, err: %s", targetURL, err)
	}

	bytes, err := ioutil.ReadAll(reader)
	if err != nil {
		return nvd, fmt.Errorf(
			"Failed to Read NVD feedfile. url: %s, err: %s", targetURL, err)
	}

	if err = xml.Unmarshal(bytes, &nvd); err != nil {
		return nvd, fmt.Errorf(
			"Failed to unmarshal. url: %s, err: %s", targetURL, err)
	}
	return nvd, nil
}
